%        File: figures.tex
%     Created: Thu Mar 24 12:00 PM 2011 P
% Last Change: Thu Mar 24 12:00 PM 2011 P
%

\documentclass[authoryear, preprint,review,12pt]{elsarticle}
%\usepackage{endfloat}
%\documentclass[final,5p,twocolumn]{elsarticle}
\bibliographystyle{elsarticle-harv}
\usepackage{lineno}
\linenumbers
\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
\usepackage{subfigure}
\usepackage[pdftex]{color}
\definecolor{darkblue}{rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0,0.5,0}
%\usepackage[pdftex, colorlinks, citecolor=darkblue,linkcolor=darkgreen]{hyperref}
\usepackage[pdftex, colorlinks]{hyperref}
\textwidth 6.75in
\oddsidemargin -0.15in
\evensidemargin -0.15in
\textheight 9in
\topmargin -0.5in
\newcommand{\ud}{\mathrm{d}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\C}{\mathrm{Cov}}
\newcommand{\V}{\mathrm{Var}}

%\graphicspath{{/home/cboettig/Documents/ucdavis/research/phylotrees/images/}}

\journal{Nature} 
\begin{document}
\begin{frontmatter}
\title{Quantifying Limits to Detection of Early Warning for Critical Transitions  }
\author[davis]{Carl Boettiger\corref{cor1}}
\ead{cboettig@ucdavis.edu}
\author[davis]{Alan M Hastings}
%\author[davis]{}
\cortext[cor1]{Corresponding author.}
\address[davis]{Center for Population Biology, University of California, Davis, United States}

%\begin{abstract}
%\end{abstract}

%\begin{keyword}
%\sep 
%\end{keyword}
\end{frontmatter}
%\include{figures.tex}
%input{figures.tex}
%\include{body.tex}
%\input{body.tex}


\appendix

\section{Method Summary}
For each dataset, parameters are estimated by maximum likelihood for a stable model and a model of a system approaching a critical transition (see Section~\ref{modelderivations}, and Section~\ref{likelihood} for the corresponding likelihood equations.)  The models are stochastic equations that can be used to simulate data under the same sampling regime as the original data, or at any specified set of time points.  Distributions are determined by a parametric bootstrap: 500 replicate datasets are simulated under both stable and deteriorating stability model (for each dataset).  For each replicate, the summary statistics (variance, autocorrelation, skew, and coefficient of variation) were computed over a sliding window equal to half the observed time, and a Kendall's correlation test applied to estimate correlation coefficient $\tau$.  The estimates of $\tau$ under the replicate simulations from the stable system and the estimates under the deteriorating stability system make up the two distributions which are integrated at different thresholds to create the ROC curves.  

The likelihood test is performed as in the parametric bootstrap; but instead of calculating summary-statistics, each simulated replicate is refit to both stable and deteriorating models to generate a difference in likelihood scores, (Cox's $\delta$ statistic, see Section~\ref{Cox}).  Note that steps such as simulating under either model or estimating each of the indicator statistics are much faster than estimating the model parameters by maximum likelihood, and that estimating the deteriorating stability model is substantially more intensive than estimating the stable model which has a closed form solution for the likelihood expression, see~\ref{likelihood}.  While in the summary-statistic approach outlined above this is estimated only once, for the likelihood approach this must be estimated 1000 times (once on each of the 500 replicates from the stable simulation and the 500 replicates from the deteriorating stability simulation), making this approach nearly 1000 times slower.  Refitting the models on each replicate, rather merely computing the likelihood under the original estimates, is essential to the power of this approach~\citep{Huelsenbeck1996}.

When replicates are created by simulation under the estimated models, we record data at the same time intervals observed in the original data (as in Fig. 3, main text).  It is straight forward to allow this interval to be different, representing the effect of increased or decreased sampling effort on the reliability-sensitivity trade-off in the ROC curve.  The warning statistics are then estimated on each set of simulated data as before.  

\section{The data sets}\label{data}
All data sets and analyses performed are included in the accompanying R package.
The simulated data sets are produced with 40 data points
sampled evenly sampled over a time interval (0,100) using an individual-based model of a system containing a saddle-node bifurcation, see~\ref{modelderivations}.  The data was then fit to the LSN model (see~\ref{modelderivations}) to compute the ROC curves.  
%The deteriorating environment was produced under the LSN model~\eqref{LSN}, with $\theta=500$, $\sigma = 5$, $R(t) = 5 - 0.04999 t$.  
%The constant data is produced using the simulation method for the constant environment model~\eqref{OU},
%with parameters estimated from the deteriorating data-set to correspond as nearly as possible to a similar system.  

The Glaciation data comes from~\citet{Petit1999}, accessible from NOAA:\\
\href{http://www.ncdc.noaa.gov/paleo/metadata/noaa-icecore-2453.html}{http://www.ncdc.noaa.gov/paleo/metadata/noaa-icecore-2453.html}.
The data is preprocessed by interpoplation and detrending to be as consistent as possible with the analysis presented in~\citet{Dakos2008},
analyzing the third glaciation event. 
The processed data consists of 121 sample points. 
The match is not exact since~\citet{Dakos2008} estimates the de-trending window size manually,
but the estimated correlations in the first-order auto-regression coefficients are in close agreement with that analysis. 
The code for processing the data from the original~\citet{Petit1999} file is included in the accompanying R package.  


The \emph{Daphnia} example comes from the chemostat ``H6'' in the experiments in \emph{Daphnia magna} populations of~\citet{Drake2010}. 
The analysis in that paper focuses on data averaged over the ensembles, though the supplement includes analysis over individual replicates. 
This individual replicates was chosen as an example of a single replicate 
that showed a statistically significant correlation in variance over window of time where critical slowing down was expected. 
Further empirical examples that have been previously investigated for early warning signs are also analyzed under our approach in~\ref{examples}.   



\section{Model Derivations}\label{modelderivations}
The sudden collapses we wish to detect are driven by bifurcations when an eigenvalue changes sign in the corresponding deterministic model,
resulting in the sudden loss of a stable state and the onset of a critical transition.
There are many possible models that can contain such a transition.
These may be highly nonlinear models with multiple attractors.
The diversity of possible models can be classified by the type of bifurcation,
such as the saddle-node bifurcation in which a stable point (node) and unstable point (saddle) collide and annihilate,
or a trans-critical bifurcation, in which a stable point becomes unstable.  
(Any dynamical systems textbook, such as \citet{Guckenheimer1983} can give a good overview of such models and bifurcations). 
In general we will be unable to estimate such models accurately if the system has only been observed near the favorable attractor
(i.e. before the critical transition occurs),
even if we knew the functional form of the model.
Fortunately the detection of early warning is based on a \emph{local} theory,
predicting changes in the dynamics near the current attractor.
Consequently, it is as unnecessary as it is impossible to estimate a fully nonlinear model,
but only the linearized model near the bifurcation.

Near the region of the bifurcation,
we can use the simple normal form equation for the bifurcation rather than the full model,
but these are still non-linear and difficult to estimate accurately from the data.
Critical slowing down is based on taking this one step further -- looking at the eigenvalue around the stable point.
This is equivalent to linearizing the normal forms of the bifurcations around the stable point.
By so doing we obtain a simple but general models we can estimate from data.
We are left with explicit, stochastic models that reflect the behavior of the more complicated systems around the stable point,
and we can see how they change as they approach the bifurcation.  

Routines for fitting each model by maximum likelihood and simulating under each model are provided in the accompanying \texttt{warningsignals} \texttt{R} package.   

\subsection{Saddle-node bifurcation}
The most frequently considered bifurcation model in early warning signals is the saddle-node bifurcation.  
In this bifurcation, a gradual decline is interrupted by a transition when a stable node and saddle node collide and annihilate, 
leaving the system to return to another attractor, if one exists.  
Because this can occur while the system is far from the other attractor, 
this transition appears particularly sudden and has been a focal concern~\citep{Scheffer2001, Scheffer2009}.  

Many models with alternative stable states considered in ecological systems can exhibit this kind of bifurcation.
For instance, a common model is one with a saturating birth rate (such as Holling Type III) and linear death rate, 
\begin{equation}
dX_t = \left( \frac{e K x^2}{X^2 + h_t^2} - e X_t - a_t\right) dt + \sigma \sqrt{ \frac{e K x^2}{X^2 + h_t^2} + e X_t + a_t} dB_t \label{ass}
\end{equation}
This model can experience a saddle-node bifurcation through increased mortality by increasing either $e$ or $a$. 
This parameterization is only an example, of course many others are possible~\citep{Scheffer2009a, Scheffer2001, Strogatz2001a, Guckenheimer1983}.  
In this case we have specified the noise dependence on $X_t$ explicitly for the influence of demographic (intrinsic) noise.
The saddle-node bifurcation has normal form
\begin{equation}
\frac{\ud x}{\ud t} = r_t- x^2.
\label{saddle-node}
\end{equation}
Linearizing this model differs from the transcritical bifurcation,
since the location of stable point moves as the bifurcation parameter changes.
Transforming the canonical form to allow for an arbitrary mean $\theta$,
the bifurcation looks like $ dx/dt = r_t- (\theta-x)^2 $, with fixed point $\hat x = \sqrt{r_t} +\theta =: \phi$,
which gives the model we refer to as the Linearized Saddle Node, LSN: 
\begin{equation}
\ud X = 2\sqrt{ r_t } (\phi - X_t)\ud t + \sigma\sqrt{\phi } \ud B_t. \label{LSN}
\end{equation}


\subsection{Transcritical Bifurcation}
Another simple bifurcation that may be common in ecological systems with alternative stable states is the transcritical bifurcation; for instance, \citet{Drake2010} induce such a bifurcation in laboratory populations of \emph{Daphnia magna} for purpose of testing early warning signals~\citep{Drake2010}.  To see how this bifurcation arises, consider for example, the stochastic Levin's model~\citep{Levins1969} (a logistic model), where $X_t$ is the number of occupied patches of some total number $K$, $c_t$ is a (time-dependent) colonization rate and $e_t$ an extinction rate (both scaled by the number of patches $K$).  
\begin{equation}
\ud X_t = \left( c_t X_t (1-X_t/K) - e_t X_t \right) \ud t + \sigma \sqrt{\frac{e_t}{c_t}} \ud B_t \label{levins},
\end{equation}
such a model can be derived from an individual-based description with or without a stochastic environment, i.e.  \citep{Kampen2007a, Nisbet2004a}.  The model contains a transcritical bifurcation when $c_t < e_t$.  The normal form of the bifurcation is
\begin{equation}
\frac{\ud x}{\ud t} = r_t x - x^2 
\label{transcritical}
\end{equation}

We can rewrite the deterministic part of the Levins model, Eq~\ref{levins} in normal form without loss of generality by taking $r_t = 1 - e_t/c_t$.  
We can linearize around the stable equilibrium value $\hat x(t) = 1 - e_t/c_t = r_t$, 
\begin{align}
\dot x &=  f(x) \approx f'(x)|_{\hat x} (x - \hat x) + \ldots, \\
 &= (r_t - 2 x_t|_{x=\hat x}) (x - \hat x), \\
 &= r_t(r_t - x).
\end{align}
Note the equilibrium time dependence arises not from the internal dynamics but from the changing parameter values.  We can express the linearized stochastic dynamics then by the time dependent mean-reverting process.  We refer to this model as the linearized transcritical bifurcation, LTC.  

\begin{equation}
\ud X_t = r_t (r_t - X_t/K) \ud t + \sigma \sqrt{1+r_t} \ud B_t \label{LTC}
\end{equation}

The pattern of the saddle-node differs from that of the transcritical in the square-root dependence on the bifurcation parameter.


\subsection{Modeling limitations}
Many models may differ substantially from~\eqref{ass} and~\eqref{levins} and yet correspond to the linearizations illustrated here.  
The linearizations have thus been constructed with enough free parameters to capture the gross features of the system,
(scaling the equilibrium position, variance, and timescale) while corresponding to the approximations of normal forms of the bifurcations.  
Despite this, there are severe limitations in formulating even a linear approximation without ample knowledge of the system. 
Note that specifying this model such that likelihoods can be calculated still requires a specification of the rate of change of $r_t$.
We assume a linear model,
\begin{equation}
r_t = r_0 - m t
\label{R_t}
\end{equation}
though the approach could easily be extended to an arbitrary model.
A gradual change can be approximately linear over the time interval of interest, and is appropriate to distinguish from a stable model.
Nonlinear rates of change in the environmental conditions will generally be harder to detect (either by summary statistics or the likelihood approach),
thus this assumption is consistent with testing the best case scenario for these methods, illustrating the intrinsic limits of these approaches.  

We also draw attention to the fact that noise scales with the mean dynamics.  
For intrinsically stochastic processes, such as demographic stochasticity, this will scale as the square root, 
while for external perturbations to the system dynamics (environmental stochasticity) this would scale linearly.
We have assumed an intrinsic noise process in both models~\eqref{LSN} and~\eqref{LTC}, 
though an alternative model with linear scaling would also be plausible.  
Note that in these models, the variance observed is determined by the ratio of the Brownian terms (in front of $\ud B_t)$
to the stabilizing force (in front of $\ud t$), so that in general larger $r_t$ will correspond to smaller system variances
regardless of this smaller correction due to the noise scaling with the mean dynamics.  
Either should perform better than the stable model if a system is slowly losing stability,
though using the better-matching model will increase the power of the approach.  

The models here are neither more nor less general than the seemingly model-free approach of summary statistics.  
We have proposed these models to match the assumptions of the summary statistics -- 
that a system is approaching a bifurcation in which a node loses stability as an eigenvalue passes through zero.  


\subsection{Stable Model}
When the system is stable, $r_t$ is constant and both models~\eqref{LSN} and~\eqref{LTC} reduce to a simple Ornstein-Uhlenbeck process, 
\begin{equation}
\ud X_t = r (\theta - X_t) \ud t + \sigma \ud B_t \label{OU}
\end{equation}
This is the continuous time analog of the first-order autoregressive model considered as a null model elsewhere (\emph{e.g.} \citet{Dakos2008, Guttal2008a}).  

\section{Likelihood calculations}\label{likelihood}
The linearization above is not only justified by the nature of and the properties we wish to estimate from the data,
but also convenient for the calculations. 
In particular, the models are specified by linear stochastic differential equations, hence their solutions are Gaussian.
The fundamental challenge is that in modeling a gradual loss of stability, the solutions are also explicitly time-dependent.
Model fitting and comparison requires we can write down the likelihood expression for each model given the time-series data.  

The probability $P(X|M)$ of the data $X$ given the model $M$ is the product of the probability of observing each point in the time series given the previous point and the length of the interval,  
\begin{equation}
\log P(X | M)=  \sum_i \log P(x_i | x_{i-1}, t_i)
\end{equation}
As the processes are Gaussian, the probability density $P$ is normally distributed and we need only calculate its first and second moments (see, \emph{e.g.}~\citep{Gardiner2009}). For any process of the form

\begin{equation}
  \ud X_t = f(X_t) \ud t + g(X_t) \ud B_t 
  \label{general}
\end{equation}

where $f(x)$ and $g(x)$ are linear functions of $x$, the solution is Gaussian with moments evolving according the the coupled ordinary differential equations:

\begin{align}
 \frac{\ud }{\ud t} E(x| M)&=  f(x) \\
\frac{\ud}{\ud t} V(x| M) &=  -\partial_x f(x) V(x|M) + g(x)^2 
  \label{general_moments}
\end{align}

For the OU process, we can solve this in closed form,~\citep{Gardiner2009}. 


\begin{align}
  E(x_i| M = \text{OU}) &= X_{i-1} e^{-r t_i} \theta \left(1 - e^{-rt_i} \right) \\
V(x_i| M = \text{OU}) &= \frac{\sigma^2}{2 r} \left(1 - e^{-2 r t_i} \right)
\label{OUsoln}
\end{align}

For the time dependent models, we have analytic forms only for the dynamical equations of these moments from equation~\eqref{general_moments}, which we must integrate numerically over each time interval.  This is responsible for most of the computational effort required of this approach.  For the linearized transcritical bifurcation:

\begin{align}
\frac{\ud }{\ud t} E(x_i| M = \text{LTC})&=  r(t)(r(t) - x_i) \\
\frac{\ud}{\ud t} V(x_i| M = \text{LTC}) &=  -2 r(t) V(x_i|M) + (1+r(t))\sigma^2 
\label{LTCsoln}
\end{align}

For the linearized saddle-node bifurcation:
\begin{align}
\frac{\ud }{\ud t} E(x_i)&=  2\sqrt{r(t)}(\sqrt{r(t)}+\theta - x_i) \\
\frac{\ud}{\ud t} V(x_i) &=  -2 \sqrt{r(t)} V(x_i) + \sigma^2 ( \sqrt{r(t)}+\theta )
\label{LSNsoln}
\end{align}

These are numerically integrated using \texttt{lsoda} routine avialble in \texttt{R} for the likelihood calculation.  

\section{The likelihood statistic}\label{Cox}
Likelihood methods form the basis of much of modern statistics, in both Frequentist and Bayesian paradigms.  
The ability evaluate likelihoods directly by computation has made it possible to treat cases that do not conform to traditional assumptions more directly.
The basis of likelihood comparisons has its roots in the Neyman-Pearson Lemma, 
which essentially asserts that comparing likelihoods is the most powerful test
of a choice between two hypotheses~\citep{Neyman1933}, which motivates
tests from the simple likelihood ratio test up through modern model adequacy methods.

The hypotheses considered here are more challenging then the original lemma, as they are composite in nature:
they specify two model forms (stable and changing stability)
but with model parameters that must be first estimated from the data.
Comparing models whose parameters have been estimated by maximum likelihood is first treated by~\citet{Cox1961, Cox1962},
and has since been developed in this simulation estimation of the null distribution~\citep{McLachlan1987}, by parametric bootstrap estimate~\citep{Efron1987}.  
Cox's $\delta$ statistic is simply the difference between the log likelihoods of these maximum likelihood estimates, defined as follows.

Let $L_0$ be the likelihood function for model 0, let $\theta_0 = \arg \max \theta_0 \in \Omega_0$, ($L_0 (\theta_0 |X)$) be the maximum likelihood estimator for $\theta_0$ given $X$, and let $L_0 = L_0 (\theta_0 |X)$; and define $L_1$ , $\theta_1$ , $L_1$ similarly for model 1. The statistic we will use is $\delta$, defined to be twice the difference in log likelihood of observing the data under the two MLE models,
$\delta = -2 (\log L_0 - \log L_1 )$

This approach has since been applied to the problem of model adequacy~\citep{Goldman1993} and model choice~\citep{Huelsenbeck1996}.  
We have extended the approach by generating the test distribution as well as a null distribution in order to compute ROC curves.  



\section{Model choice and model adequacy}\label{modelchoice}
Any approach is only as good as its underlying assumptions.  In this paper we have leveraged the machinery of likelihood-based approaches to both evaluate the performance of existing methods and provide a statistical test that more closely matches the assumptions of the basic theory.  For this approach to be useful, the data must still be well-approximated by the models used.  We have provided two different simple bifurcation models that could each represent the hypothesis of gradual decline in stability: linearized transcritical bifurcation and the linearized saddle-node bifurcation; Equations~\eqref{LTC},~\eqref{LSN}.  As they both represent a gradual loss of stability either can be used, but the test will be most powerful by using the one that best corresponds to the data.  While knowledge of the system alone can suggest this (such as in the case of~\citet{Drake2010}), it is straight forward to compare the models against each other using the same likelihood framework, as illustrated in Figure~\ref{fig:modelchoice} for several empirical examples.  

\begin{figure}[ht]
\begin{center}
\includegraphics[width=.25\textwidth]{deut_modelchoice.png}
\includegraphics[width=.25\textwidth]{caco3_modelchoice.png}
\end{center}
\caption{Model }
\label{fig:modelchoice}
\end{figure}

As discussed in the text, many systems may not correspond well to any of the simple dynamics upon which the detection of early warning signals is based~\citep{Hastings2010}.  Such dynamics can be particularly misleading for current methods based on summary statistics.  The Monte Carlo bootstrapping of the likelihood approach can be applied as a test of model adequacy, as illustrated in \citet{Goldman1993} and discussed in \citet{Sullivan2005b} in a phylogenetics context. If the observed likelihood ratio falls far from the distributions estimated under either model, the model is likely an inadequate description and subsequent inference may be misleading.  


% Choosing between models

\section{R package tutorial}
We provide an R package with simple implementation of the methods described here.  The package is available from: \href{https://github.com/cboettig/warningsignals/archives/master}{https://github.com/cboettig/warningsignals/archives/master}, where it will be actively maintained and developed.  The package takes an R time-series object (or, for unevenly spaced data -- a matrix or data-frame with sample times in the first column and observations in the second) as input and performs the likelihood-based analysis:

\begin{verbatim}
> require(warningsignals)
>
> # Load a sample dataset
> data(glaciationIII)
>
> # Fit constant (OU) model and a stability-loss model (LSN);
> models <- fit_models(glaciationIII, 'LSN')
>
> results <- montecarlotest(models$const, models$timedep, n=2000, cpu=16)
> plot(results)
\end{verbatim}

The package also performs the computationally faster but less powerful summary-statistic tests, computes the resulting distributions under the different hypotheses and the corresponding ROC curves.  The package allows a variety of correlation tests (Pearson's, Kendall's, Spearman's) as the measure of increase, and a variety of summary statistics (variance, autocorrelation, skew, coeffient of variation).  Copies of data used in the paper are provided as examples.  The Monte Carlo methods can take advantage of multiple core processors or clusters by specifying the number of CPUs in the function argument.   Full documentation of functions and data are provided in the package, as well as example scripts which replicate the analyses presented in the paper.   

\section{Examples}\label{examples}
In this section we illustrate the main results of the paper in three further empirical datasets from the climate record.  


\pagebreak

\section*{ }%bibliography
\bibliography{/home/cboettig/Documents/bibliographies/library.bib}

\end{document}



